<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset='utf8'>
	<link rel="stylesheet" type="text/css" href="style.css">
	<title>Matthew Jörke – Logistic Regression & Information Theory</title>
	<link href="https://fonts.googleapis.com/css?family=IBM+Plex+Mono:300,300i&display=swap" rel="stylesheet">
	</script>
	<script type="text/javascript" 
			src="js/macy.min.js">
	</script>
	<script type="text/javascript" src='js/content.js' defer></script>
</head>
<body>
	<div class='container'>
		<a href="index.html">
			<div class="backPane"></div>
		</a>
		
		<div class='content'>
			<div class="contentRow">
				<div class='title'>
					<span class='sans'>
						Logistic Regression & <br>Information Theory
					</span>
				</div>
			</div>
			<div class="contentRow">
				<div class='text'>
					In fall 2018, I was taking courses on 
					<a class='bodyLink' href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EECS127/">convex optimization</a>
					and on 
					<a class='bodyLink' href="http://people.ischool.berkeley.edu/~dbamman/nlp18.html">natural language processing</a>. When learning about Maximum Entropy Markov Models, I was curious how the term "maximum entropy" came about. Our 
					<a class='bodyLink' href="http://people.ischool.berkeley.edu/~dbamman/nlp18.html">NLP textbook's</a> explanation was unsatisfying at best,
					<blockquote>
						'Maximum entropy model' is an outdated name for logistic regression
					</blockquote>
					so I decided to do some research of my own. Check out the PDF below to learn more about the connections between information theory, optimization theory, and logistic regression. 
				</div>
			</div>
			<div class='contentRow' style='margin-top: 20px'>
				<a href='files/logreg.pdf'>
					<span class='downloadBox'>
						↓ Download
					</span>
				</a>
			</div>
			
		</div>
		</div>

	</div>
	<div class='row footer'>
		<div class="gutter">
		</div>
		<div class='content text' id='attr'>
			© Matthew Jörke, 2019
		</div>
	</div>
</body>
</html>